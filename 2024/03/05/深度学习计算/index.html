<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<link href="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.10.2/katex.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>深度学习计算 | DayDreamer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="层块的创建，文件的读取，参数的管理与使用GPU">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习计算">
<meta property="og:url" content="https://daydreamerh.github.io/2024/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/index.html">
<meta property="og:site_name" content="DayDreamer">
<meta property="og:description" content="层块的创建，文件的读取，参数的管理与使用GPU">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daydreamerh.github.io/2024/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/banner.jpg">
<meta property="article:published_time" content="2024-03-05T05:50:28.000Z">
<meta property="article:modified_time" content="2024-03-05T14:08:58.454Z">
<meta property="article:author" content="DayDreamer">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://daydreamerh.github.io/2024/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/banner.jpg">
  
    <link rel="alternate" href="/atom.xml" title="DayDreamer" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>DayDreamer </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/avatar.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">PY H </div>
      <div class="dot"></div>
      <div class="subtitle"> </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/DaydreamerH" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
                计算机操作系统
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">
                杂七杂八
                <div class="category-count">7</div>
            </a>
        
            <a class="category-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">
                PyTorch学习
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/">
                套接字编程
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/C-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/">
                C++快速入门
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%94%BF%E6%B2%BB%E8%AF%BE%E5%A4%8D%E4%B9%A0/">
                政治课复习
                <div class="category-count">8</div>
            </a>
        
            <a class="category-link" href="/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/">
                项目开发
                <div class="category-count">3</div>
            </a>
        </div>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-深度学习计算" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      
      
      
      
      
      
      <a class="article-gallery-img" rel="gallery_cm1xhgw9z002yqcvj547x3m8d">
        <img src="/2024/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/banner.jpg" itemprop="image">
      </a>
    
  </div>
</div>

   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        深度学习计算
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-03-05T05:50:28.000Z" itemprop="datePublished">2024-03-05</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">PyTorch学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            6.2k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="层和块"><a class="markdownIt-Anchor" href="#层和块"></a> 层和块</h1>
<ul>
<li>我们首先学习的是具有单一输出的线性模型
<ul>
<li>接受一些输入</li>
<li>生成相应的标量输出</li>
<li>具有一组相关参数，更新这些参数可以优化某目标函数</li>
</ul>
</li>
<li>然后考虑具有多个输出的网络，利用向量化算法来描述整层神经元
<ul>
<li>接收一组输入</li>
<li>生成相应的输出</li>
<li>由一组可调整参数描述</li>
</ul>
</li>
<li>随后我们学习率多层感知机，整个模型及其组成层都是上述架构。</li>
</ul>
<p>事实证明，研究讨论比单个层大但比整个模型小的组件更有价值。</p>
<blockquote>
<p>例如，ResNet-152架构就有数百层，这些层是由层组的重复模式组成</p>
</blockquote>
<p>为了实现这些复杂的网络，引入了神经网络块的概念。</p>
<p>块可以描述单个层、由多个层组成的组件或整个模型本身。</p>
<p>使用块的好处是可以将一些块组成更大的组件，这一过程往往是递归的。</p>
<p>从编程的角度来看，块由类表示，类的任何子类都必须定义一个将其输入转换为输出的前向传播的函数，并且必须存储任何必须的参数。</p>
<h2 id="自定义块"><a class="markdownIt-Anchor" href="#自定义块"></a> 自定义块</h2>
<p>块必须提供以下功能：</p>
<ul>
<li>将输入数据作为其前向传播函数的参数</li>
<li>通过前向传播函数来生成输出。</li>
<li>计算其输出关于输入的梯度，可通过反向传播函数进行访问</li>
<li>存储和访问前向传播计算所需的参数</li>
<li>根据需要初始化模型参数</li>
</ul>
<p>此处定义一个块，其输入有样本有20维特征，经过256个隐藏单元组成的隐藏层后，在输出层输出10维结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from torch.nn import functional as F<br>class MLP(nn.Module):<br>    &#x27;&#x27;&#x27;该类继承了表示块的类，我们只需要提供构造函数与前向传播函数&#x27;&#x27;&#x27;<br>    def __init__(self):<br>        # 调用父类构造函数执行必要的初始化<br>        # 这样，在类的实例化时也可以指定其他函数参数<br>        super().__init__()<br>        self.hidden = nn.Linear(20, 256)<br>        self.out = nn.Linear(256, 10)<br>    <br>    # 定义模型的前向传播<br>    def forward(self, X):<br>        return self.out(F.relu(self.hidden(X))) <br></code></pre></td></tr></table></figure>
<p>关于使用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.rand(2, 20)<br>net = MLP()<br>net(X)<br></code></pre></td></tr></table></figure>
<p>块的一个主要优点在于其多功能性，我们可以子类化块，以创建层、整个模型或具有中等复杂度的各种组件。</p>
<h2 id="顺序块"><a class="markdownIt-Anchor" href="#顺序块"></a> 顺序块</h2>
<p>接下来学习Sequential类是如何工作的。</p>
<p>为了构建我们自己的简化的Sequential，只需要定义下面两个关键函数：</p>
<ul>
<li>将块逐个追加到列表中的函数</li>
<li>前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class MySequential(nn.Module):<br>    def __init__(self, *args):<br>        super().__init__()<br>        for idx, module in enumerate(args):<br>            # 这里，module是Module子类的一个实例，我们把它保存至Module类的成员变量_modules中。<br>            # _modules的原型是OrderedDict<br>            self._modules[str(idx)] = module<br>    <br>    def forward(self, X):<br>        for block in self._modules.values():<br>            X = block(X)<br>        return X<br></code></pre></td></tr></table></figure>
<p>__init__函数将每个块逐个添加到有序字典_modules中。</p>
<p>使用_modules的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典查找需要初始化参数的子块。</p>
<p>当MySequential的前向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))<br>net(X)<br></code></pre></td></tr></table></figure>
<h2 id="在前向传播函数中执行代码"><a class="markdownIt-Anchor" href="#在前向传播函数中执行代码"></a> 在前向传播函数中执行代码</h2>
<p>并不是所有的结构都是简单的顺序架构。</p>
<p>当需要更强的灵活性时，我们需要定义自己的块。</p>
<p>有时，我们可能希望合并既不是上一层的结果也不是可更新参数的项，我们称之为常熟参数。</p>
<p>例如，我们需要一个计算函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x,y)=cw^Tx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord mathdefault">c</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span></span></span></span>的层，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>是输入，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>是参数，而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span>是某个在优化过程中没有更新的指定常量。</p>
<p>实现如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class FixedHiddenMLP(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        # 不计算梯度的随机权重参数<br>        self.rand_weight = torch.rand((20, 20), requires_grad=False)<br>        self.lin = nn.Linear(20, 20)<br>    <br>    def forward(self, X):<br>        X = self.lin(X)<br>        X = F.relu(torch.mm(X, self.rand_weight)+1)<br>        return X<br></code></pre></td></tr></table></figure>
<h1 id="参数管理"><a class="markdownIt-Anchor" href="#参数管理"></a> 参数管理</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>from torch import nn<br><br>net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))<br>X = torch.rand(size=(2, 4))<br>net(X)<br></code></pre></td></tr></table></figure>
<h2 id="参数访问"><a class="markdownIt-Anchor" href="#参数访问"></a> 参数访问</h2>
<p>当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层。</p>
<p>模型就像一个列表，每层的参数都在其属性中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">print(net[2].weight.shape)<br></code></pre></td></tr></table></figure>
<h3 id="目标参数"><a class="markdownIt-Anchor" href="#目标参数"></a> 目标参数</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">print(net[2].bias)<br>print(net[2].bias.data)<br>net[2].weight.data == None<br></code></pre></td></tr></table></figure>
<h3 id="一次性访问所有参数"><a class="markdownIt-Anchor" href="#一次性访问所有参数"></a> 一次性访问所有参数</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">print(*[(name, param.shape) for name, param in net[0].named_parameters()])<br><br>print(*[(name, param.shape) for name, param in net.named_parameters()])<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net.state_dict()[&#x27;2.bias&#x27;].data<br></code></pre></td></tr></table></figure>
<h3 id="从嵌套块中收集参数"><a class="markdownIt-Anchor" href="#从嵌套块中收集参数"></a> 从嵌套块中收集参数</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def block1():<br>    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),<br>    nn.Linear(8, 4), nn.ReLU())<br><br>def block2():<br>    net = nn.Sequential()<br>    for i in range(4):<br>        net.add_module(f&#x27;block&#123;i&#125;&#x27;, block1())<br>    return net<br><br>rgnet = nn.Sequential(block2(), nn.Linear(4, 1))<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">rgnet[0][1][0].bias.data<br></code></pre></td></tr></table></figure>
<h2 id="参数初始化"><a class="markdownIt-Anchor" href="#参数初始化"></a> 参数初始化</h2>
<h3 id="内置初始化"><a class="markdownIt-Anchor" href="#内置初始化"></a> 内置初始化</h3>
<p>我们首先调用内置的初始化器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def init_normal(m):<br>    if type(m) == nn.Linear:<br>        nn.init.normal_(m.weight, mean =0, std=0.01)<br>        nn.init.zeros_(m.bias)<br>net = nn.Sequential(nn.Linear(20, 10))<br>net[0].weight.data, net[0].bias.data<br>net.apply(init_normal)<br>net[0].weight.data, net[0].bias.data<br></code></pre></td></tr></table></figure>
<h3 id="自定义初始化"><a class="markdownIt-Anchor" href="#自定义初始化"></a> 自定义初始化</h3>
<p>有时，深度学习框架没有提供我们所需要的初始化方法。</p>
<p>例如，我们使用以下分布为任意权重参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>定义初始化方法。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mtext> </mtext><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>U</mi><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mn>10</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">可</mi><mi mathvariant="normal">能</mi><mi mathvariant="normal">性</mi><mfrac><mn>1</mn><mn>4</mn></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">可</mi><mi mathvariant="normal">能</mi><mi mathvariant="normal">性</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>U</mi><mo stretchy="false">(</mo><mo>−</mo><mn>10</mn><mo separator="true">,</mo><mo>−</mo><mn>5</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">其</mi><mi mathvariant="normal">他</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">w~\begin{cases}
    U(5, 10), 可能性\frac{1}{4}\\
    0, 可能性\frac{1}{2}\\
    U(-10, -5), 其他
\end{cases}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.32em;vertical-align:-1.9099999999999997em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace nobreak"> </span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35002em;"><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-3.1500100000000004em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.30001em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.60002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8500199999999998em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">能</span><span class="mord cjk_fallback">性</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">能</span><span class="mord cjk_fallback">性</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-1.5300000000000002em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord">5</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">其</span><span class="mord cjk_fallback">他</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9099999999999997em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def my_init(m):<br>    if type(m) == nn.Linear():<br>        nn.init.uniform_(m.weight, -10, 10)<br>        m.weight.data *= m.weight.data.abs()&gt;=5<br></code></pre></td></tr></table></figure>
<h2 id="参数绑定"><a class="markdownIt-Anchor" href="#参数绑定"></a> 参数绑定</h2>
<p>有时，我们希望在多个层间共享参数，我们可以定义一个稠密层，然后使用这个稠密层来设置另一个层的参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">shared = nn.Linear(20, 20)<br>net = nn.Sequential(nn.Linear(10, 20), nn.ReLU(),<br>shared, nn.ReLU(),<br>shared, nn.ReLU())<br><br>net[2].weight.data == net[4].weight.data<br>net[2].weight.data = torch.rand(size=(20, 20))<br>net[2].weight.data == net[4].weight.data<br></code></pre></td></tr></table></figure>
<h2 id="延后初始化"><a class="markdownIt-Anchor" href="#延后初始化"></a> 延后初始化</h2>
<p>到目前为止，我们建立网络时忽略了需要做的以下事情：</p>
<ul>
<li>我们定义了网络架构，但没有指定输入的维度</li>
<li>我们添加层时，没有指定前一层的输出维度</li>
<li>我们在初始化参数时，甚至没有信心来确定模型应该包含多少个参数</li>
</ul>
<p>这里的诀窍是框架的延迟初始化，即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。</p>
<h1 id="自定义层"><a class="markdownIt-Anchor" href="#自定义层"></a> 自定义层</h1>
<p>深度学习成功背后的一个因素是神经网络的灵活性，我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的结构。</p>
<h2 id="不带参数的层"><a class="markdownIt-Anchor" href="#不带参数的层"></a> 不带参数的层</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from torch import nn<br>class CenteredLayer(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>    <br>    def forward(self, X):<br>        return X - X.mean()<br><br>layer = CenteredLayer()<br><br>net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())<br>Y = net(torch.rand(4, 8))<br>Y.mean()<br></code></pre></td></tr></table></figure>
<h2 id="带参数的层"><a class="markdownIt-Anchor" href="#带参数的层"></a> 带参数的层</h2>
<p>我们实现自定义版本的全连接层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class MyLinear(nn.Module):<br>    def __init__(self, in_units, units):<br>        super().__init__()<br>        self.weight = nn.Parameter(torch.randn(in_units, units))<br>        self.bias = nn.Parameter(torch.randn(units, ))<br>    def forward(self, X):<br>        linear = torch.matmul(X, self.weight.data) + self.bias.data<br>        return F.relu(linear)<br><br>linear = MyLinear(5, 3)<br>linear(torch.rand(2, 5))<br></code></pre></td></tr></table></figure>
<h1 id="读写文件"><a class="markdownIt-Anchor" href="#读写文件"></a> 读写文件</h1>
<h2 id="加载和保存张量"><a class="markdownIt-Anchor" href="#加载和保存张量"></a> 加载和保存张量</h2>
<p>对于单个张量，我们可以直接调用load和save函数分别读写它们。这两个函数都要求我们提供名称，save要求将要保存的变量作为输入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">x = torch.arange(4)<br>x<br>torch.save(x, &#x27;x-file&#x27;)<br>x2 = torch.load(&#x27;x-file&#x27;)<br>x2<br></code></pre></td></tr></table></figure>
<p>我们可以存储一个张量列表，随后将它们读回内存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">y = torch.zeors(4)<br>torch.save([x, y],&#x27;x-file&#x27;)<br>x2, y2 = torch.load(&#x27;x-file&#x27;)<br>x2, y2<br></code></pre></td></tr></table></figure>
<p>我们甚至可以读取或写入从字符串映射到张量的字典。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">mydict = &#123;&#x27;x&#x27;:x, &#x27;y&#x27;:y&#125;<br>torch.save(mydict,&#x27;mydict&#x27;)<br>mydict2 = torch.load(&#x27;mydict&#x27;)<br>mydict2<br></code></pre></td></tr></table></figure>
<h2 id="加载和保存模型参数"><a class="markdownIt-Anchor" href="#加载和保存模型参数"></a> 加载和保存模型参数</h2>
<p>深度学习框架提供了内置函数来保存和加载整个网络。</p>
<p>注意，这将保存模型的参数而不是整个模型。因此，为了恢复模型，我们需要用代码生成架构，然后从磁盘加载参数。</p>
<p>首先我们从熟悉的多层感知机开始尝试。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class MLP(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.hidden = nn.Linear(20, 256)<br>        self.out = nn.Linear(256, 10)<br>    <br>    def forward(self, X):<br>        return self.out(F.relu(self.hidden(X)))<br><br>net = MLP()<br></code></pre></td></tr></table></figure>
<p>接下来，我们将模型的参数保存至一个叫mlp.params的文件中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">torch.save(net.state_dict(),&#x27;mlp.params&#x27;)<br></code></pre></td></tr></table></figure>
<p>为了恢复模型，我们实例化了原始多层感知机模型的一个备份。这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">clone = MLP()<br>clone.load_state_dict(torch.load(&#x27;mlp.params&#x27;))<br>clone.eval()<br></code></pre></td></tr></table></figure>
<h1 id="gpu"><a class="markdownIt-Anchor" href="#gpu"></a> GPU</h1>
<h2 id="计算设备"><a class="markdownIt-Anchor" href="#计算设备"></a> 计算设备</h2>
<p>我们可以指定用于存储和计算的设备。默认情况下张量在内存中创建，使用CPU计算。</p>
<p>在PyTorch中，CPU和GPU可以用<code>torch.device('cpu')</code>和<code>torch.device('GPU')</code>表示。</p>
<p>如果有多个GPU设备，可以使用<code>torch.device(f'cuda:&#123;i&#125;')</code>来比表示第i块GPU。</p>
<p>查询GPU数量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">torch.cuda.device_count()<br></code></pre></td></tr></table></figure>
<p>现在定义两个方便的函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def try_gpu(i=0): #@save<br>    if torch.cuda.device_count() &gt;= i+1:<br>        return torch.device(f&#x27;cuda:&#123;i&#125;&#x27;)<br>    return torch.device(&#x27;cpu&#x27;)<br><br>def try_all_gpus(): #@save<br>    devices = [torch.device(f&#x27;cuda:&#123;i&#125;&#x27;) for i in range(torch.cuda.device_count())]<br>    return devices if devices else [torch.device(&#x27;cpu&#x27;)]<br></code></pre></td></tr></table></figure>
<h2 id="张量与gpu"><a class="markdownIt-Anchor" href="#张量与gpu"></a> 张量与GPU</h2>
<p>我们可以查询张量所在的设备。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">x = torch.rand(2, 3)<br>x.device<br></code></pre></td></tr></table></figure>
<h3 id="存储在gpu上"><a class="markdownIt-Anchor" href="#存储在gpu上"></a> 存储在GPU上</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.ones(2, 3, device = try_gpu())<br>X<br></code></pre></td></tr></table></figure>
<h3 id="复制"><a class="markdownIt-Anchor" href="#复制"></a> 复制</h3>
<p>跨GPU复制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Z = X.cuda(1)<br>print(X)<br>print(Z)<br></code></pre></td></tr></table></figure>
<p>对于两个GPU上的变量，不能简单地将其相加，因为运行时引擎不知道该怎么做，它在同一设备上找不到数据而导致失败。</p>
<h2 id="神经网络与gpu"><a class="markdownIt-Anchor" href="#神经网络与gpu"></a> 神经网络与GPU</h2>
<p>类似地，神经网络模型可以指定设备，下面的代码将模型参数放在GPU上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = nn.Sequential(nn.Linear(3, 1))<br>net = net.to(device=try_gpu())<br></code></pre></td></tr></table></figure>
<p>只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2024/03/06/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
      title="卷积神经网络"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        卷积神经网络
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2024/03/03/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%EF%BC%88%E4%B8%80%EF%BC%89/"
      title="计算机视觉（一）"
     >

    <p class="title-text">
      
        计算机视觉（一）
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">留言 </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":"cad9Un1DX7ZRoGqWGBbgsdvR-gzGzoHsz","appKey":"KdaYDEn7evWoAgBjACdAKPdK","placeholder":"(>_<)","pageSize":10,"highlight":true,"serverURLs":"https://cad9un1d.lc-cn-n1-shared.com","el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 PY H<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
