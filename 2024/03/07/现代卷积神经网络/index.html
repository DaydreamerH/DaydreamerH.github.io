<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<link href="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.10.2/katex.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>现代卷积神经网络 | DayDreamer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="AlexNet VGG NiN GoogLeNet BatchNorm ResNet原理介绍与实现">
<meta property="og:type" content="article">
<meta property="og:title" content="现代卷积神经网络">
<meta property="og:url" content="https://daydreamerh.github.io/2024/03/07/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="DayDreamer">
<meta property="og:description" content="AlexNet VGG NiN GoogLeNet BatchNorm ResNet原理介绍与实现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daydreamerh.github.io/2024/03/07/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/banner.jpg">
<meta property="article:published_time" content="2024-03-07T08:18:02.000Z">
<meta property="article:modified_time" content="2024-03-10T13:37:40.336Z">
<meta property="article:author" content="DayDreamer">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="cv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://daydreamerh.github.io/2024/03/07/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/banner.jpg">
  
    <link rel="alternate" href="/atom.xml" title="DayDreamer" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>DayDreamer </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/avatar.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">PY H </div>
      <div class="dot"></div>
      <div class="subtitle"> </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/DaydreamerH" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
                计算机操作系统
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">
                PyTorch学习
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">
                杂七杂八
                <div class="category-count">7</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/">
                套接字编程
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%94%BF%E6%B2%BB%E8%AF%BE%E5%A4%8D%E4%B9%A0/">
                政治课复习
                <div class="category-count">6</div>
            </a>
        
            <a class="category-link" href="/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/">
                项目开发
                <div class="category-count">3</div>
            </a>
        </div>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-现代卷积神经网络" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      
      
      
      
      
      
      <a class="article-gallery-img" rel="gallery_clxk1zx16002wnov14caa4hvr">
        <img src="/2024/03/07/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/banner.jpg" itemprop="image">
      </a>
    
  </div>
</div>

   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        现代卷积神经网络
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-03-07T08:18:02.000Z" itemprop="datePublished">2024-03-07</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">PyTorch学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            15k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cv/" rel="tag">cv</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="深度卷积神经网络alexnet"><a class="markdownIt-Anchor" href="#深度卷积神经网络alexnet"></a> 深度卷积神经网络（AlexNet）</h1>
<p>2012年，AlexNet首次证明了学习到的特征可以超越手动设计的特征。</p>
<p>AlexNet的架构与LeNet非常相似。</p>
<p>在这里我们介绍的架构是稍微精简版的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。</p>
<p>AlexNet与LeNet的差异：</p>
<ul>
<li>AlexNet比相对较小的LeNet-5要深得多。AlexNet由8层组成：5个卷积层、2个全连接隐藏层和1个全连接输出层。</li>
<li>AlexNet使用ReLU而不是sigmoid作为其激活函数</li>
</ul>
<blockquote>
<p>考虑到电脑捞逼，又不想花钱，所以这里用resize后的Fashion-MNIST做数据集。</p>
</blockquote>
<h2 id="模型设计"><a class="markdownIt-Anchor" href="#模型设计"></a> 模型设计</h2>
<ul>
<li>在AlexNet的第一层，卷积窗口的形状是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>11</mn><mo>×</mo><mn>11</mn></mrow><annotation encoding="application/x-tex">11\times 11</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">1</span></span></span></span>。
<ul>
<li>由于ImageNet中大多数图像高宽都比MNIST图像的大十倍以上，因此需要一个更大的卷积窗口来捕获目标</li>
</ul>
</li>
<li>AlexNet的卷积通道数是LeNet的十倍</li>
<li>在最后一个卷积层后有两个全连接层，分别有4096个输出，这两个全连接层接近1GB的参数
<ul>
<li>由于早期GPU显存有限，原始的AlexNet采用了双数据流设计。</li>
</ul>
</li>
</ul>
<h2 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h2>
<p>AlexNet将激活函数改为ReLU，一方面函数的计算更简单；另一方面，当使用不同的参数初始化时，ReLU激活函数使训练模型更加容易。</p>
<h2 id="容量控制和预处理"><a class="markdownIt-Anchor" href="#容量控制和预处理"></a> 容量控制和预处理</h2>
<p>AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。</p>
<p>为了进一步扩增数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色，这使得模型更健壮，更大的样本有效地减少了过拟合。</p>
<h2 id="模型搭建"><a class="markdownIt-Anchor" href="#模型搭建"></a> 模型搭建</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>from torch import nn<br>from d2l import torch as d2l<br><br>net = nn.Sequential(<br>    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=3, stride=2),<br>    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=3, stride=2), <br>    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),<br>    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),<br>    nn.Conv2d(384, 256, kernel_size=3, stride=2), nn.ReLU(),<br>    nn.Flatten(),<br>    nn.Linear(6400, 4096), nn.ReLU(),<br>    nn.Dropout(p=0.5),<br>    nn.Linear(4096, 4096), nn.ReLU(),<br>    nn.Dropout(p=0.5),<br>    nn.Linear(4096, 10)<br>)<br></code></pre></td></tr></table></figure>
<p>检查：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.rand(size=(1,1,224,224))<br>for layer in net:<br>    X = layer(X)<br>    print(layer.__class__.__name__, X.shape)<br></code></pre></td></tr></table></figure>
<h2 id="读取数据集"><a class="markdownIt-Anchor" href="#读取数据集"></a> 读取数据集</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from torchvision import transforms<br>from torchvision import datasets<br>from torch.utils import data<br>def load_data_fashion_mnist(batch_size, resize=None):<br>    trans = [transforms.ToTensor()]<br><br>    if resize:<br>        trans.insert(0, transforms.Resize(resize))<br>    <br>    trans = transforms.Compose(trans)<br><br>    mnist_train = datasets.FashionMNIST(&#x27;../data&#x27;, train=True, transform=trans, download=True)<br>    mnist_test = datasets.FashionMNIST(&#x27;../data&#x27;, train=False, transform=trans, download=True)<br><br>    return data.DataLoader(mnist_train, batch_size, shuffle=True), data.DataLoader(mnist_test, batch_size)<br><br>batch_size = 128<br>train_iter, test_iter = load_data_fashion_mnist(batch_size, 224)<br></code></pre></td></tr></table></figure>
<h2 id="训练alexnet"><a class="markdownIt-Anchor" href="#训练alexnet"></a> 训练AlexNet</h2>
<h1 id="使用块的网络vgg"><a class="markdownIt-Anchor" href="#使用块的网络vgg"></a> 使用块的网络（VGG）</h1>
<p>虽然AlexNet证明深层网络卓有成效，但它没有提供一个通用的模板来指导，后续的研究人员设计新的网络。</p>
<h2 id="vgg块"><a class="markdownIt-Anchor" href="#vgg块"></a> VGG块</h2>
<p>经典卷积神经网络的基本组成部分是下面这个序列：</p>
<ul>
<li>带填充以保持分辨率的卷积层</li>
<li>非线性激活函数</li>
<li>汇聚层</li>
</ul>
<p>而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间降采样的最大汇聚层。</p>
<p>在最初的VGG论文中，作者使用了带有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>卷积核，填充为1的卷积层，以及带有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>汇聚窗口、步幅为2的最大汇聚层。</p>
<p>VGG块的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def vgg_block(num_convs, in_channels, out_channels):<br>    layers = []<br>    for _ in range(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layer.append(nn.MaxPool2d(kernel_size=2, stride=2))<br>    return nn.Sequential(*layers)<br></code></pre></td></tr></table></figure>
<h2 id="vgg网络"><a class="markdownIt-Anchor" href="#vgg网络"></a> VGG网络</h2>
<p>与AlexNet、LeNet一样，VGG网络可以分为两部分，第一部分主要由卷积层和汇聚层组成，第二部分由全连接组成。</p>
<p>原始VGG网络有5个卷积块，其中前2个块各包含一个卷积层，后3个块各包含两个卷积层。</p>
<p>第一块有64个输出通道，后续每个块都将输出通道数翻倍，直到输出通道数达到512.</p>
<p>由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def vgg(conv_arch):<br>    conv_blks = []<br>    in_channels = 1<br><br>    for (num_convs, out_channels) in conv_arch:<br>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br><br>    return nn.Sequential(<br>        *conv_blks, nn.Flatten(),<br>        nn.Linear(out_channels*7*7, 4096), nn.ReLU(), nn.Dropout(p=0.5),<br>        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),<br>        nn.Linear(4096, 10)<br>    )<br><br>conv_arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))<br>net = vgg(conv_arch)<br></code></pre></td></tr></table></figure>
<p>检查：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.rand(size=(1,1,224,224))<br>for layer in net:<br>    X = layer(X)<br>    print(layer.__class__.__name__, X.shape)<br></code></pre></td></tr></table></figure>
<h2 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h2>
<p>由于VGG-11比AlexNet的计算量更大，所以我们将缩减通道数，将其用于Fashion-MNIST数据集。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from d2l import torch as d2l<br><br>ratio = 4<br>small_conv_arch = [(pair[0], pair[1]//ratio)for pair in conv_arch]<br>net = vgg(small_conv_arch)<br><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)<br><br>def train(net, train_iter, test_iter, num_epochs, device, lr):<br>    loss = nn.CrossEntropyLoss()<br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br><br>    def init_weights(m):<br>        if type(m) == nn.Linear or type(m) == nn.Conv2d:<br>            nn.init.xavier_uniform_(m.weight)<br>    <br>    net.apply(init_weights)<br>    net.to(device)<br><br>    for epoch in range(num_epochs):<br>        train_loss = 0<br>        n_train = 0<br>        for X,y in train_iter:<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            <br>            optimizer.zero_grad()<br>            l.backward()<br>            optimizer.step()<br><br>            train_loss += l*X.shape[0]<br>            n_train += X.shape[0]<br>            <br>        print(f&#x27;epoch: &#123;epoch+1&#125;&#x27;)<br>        print(f&#x27;train loss: &#123;train_loss/n_train&#125;&#x27;)<br>        with torch.no_grad():<br>            test_acc = 0<br>            n_test = 0<br>            for X,y in test_iter:<br>                X, y = X.to(device), y.to(device)<br>                y_hat = net(X)<br>                y_hat = y_hat.argmax(axis = 1)<br>                cmp = y_hat.type(y.dtype) == y<br>                test_acc += cmp.type(y.dtype).sum()<br>                n_test += len(y)<br>            print(f&#x27;test acc: &#123;test_acc/n_test&#125;&#x27;)<br><br>train(net, train_iter, test_iter, num_epochs, &#x27;cuda:0&#x27;, lr)<br></code></pre></td></tr></table></figure>
<h1 id="网络中的网络nin"><a class="markdownIt-Anchor" href="#网络中的网络nin"></a> 网络中的网络（NiN）</h1>
<p>LeNet、AlexNet和VGG都有共同的设计模式，通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征表征进行处理。</p>
<p>其中AlexNet与VGG的改进主要在于如何扩大和加深这两个模块。</p>
<p>有时可以想象在早期使用全连接层，但如果这样做，可能会完全放弃表征的空间结构。</p>
<p>NiN提供了一个非常简单的方法：在每个像素的通道上分别使用多层感知机。</p>
<h2 id="nin块"><a class="markdownIt-Anchor" href="#nin块"></a> NiN块</h2>
<p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。</p>
<p>全连接层的输入和输出通常是分别对应于样本和特征的二位张量。</p>
<p>NiN的想法是在每个像素的位置应用一个全连接层，如果将权重连接到每个空间位置，我们可以将其视为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>的卷积层。</p>
<p>从另一个角度看，是将空间维度的每个像素视为单个样本，将通道视为不同的维度。</p>
<p>NiN块以一个普通卷积层开始，后面两个是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层，这两层充当带有ReLU激活函数的逐像素全连接层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def nin_block(in_channels, out_channels, kernel_size, strides, padding):<br>    return nn.Sequential(<br>        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=strides, padding=padding),<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, kernel_size=1),<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, kernel_size=1),<br>        nn.ReLU()<br>    )<br></code></pre></td></tr></table></figure>
<h2 id="nin网络"><a class="markdownIt-Anchor" href="#nin网络"></a> NiN网络</h2>
<p>NiN网络使用窗口形状为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>11</mn><mo>×</mo><mn>11</mn></mrow><annotation encoding="application/x-tex">11\times 11</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">1</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的卷积层，输出通道数与AlexNet相同，而每个NiN块后有一个最大汇聚层，汇聚窗口形状为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>，步幅为2。</p>
<p>NiN与AlexNet之间的一个显著区别是NiN完全取消了全连接层，而使用了一个NiN块，其输出通道等于标签类别数，最后放一个全局汇聚层，生成一个对数几率。</p>
<p>NiN设计的一个优点是，它显著降低了模型所需的参数，然而这种设计有时会增加训练模型的时间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = nn.Sequential(<br>    nin_block(1, 96, 11, 4, 0),<br>    nn.MaxPool2d(kernel_size=3, stride=2),<br>    nin_block(96, 256, kernel_size=5, strides=1, padding=1),<br>    nn.MaxPool2d(kernel_size=3, stride=2),<br>    nin_block(256, 384, kernel_size=3, strides=1, padding=1),<br>    nn.MaxPool2d(kernel_size=3, stride=2),<br>    nin_block(384, 10, kernel_size=3, padding=1, strides=1),<br>    nn.AdaptiveAvgPool2d((1, 1)),<br>    nn.Flatten()<br>)<br></code></pre></td></tr></table></figure>
<h2 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h2>
<p><s>与以前一样，不想写了</s></p>
<h1 id="含并行连接的网络googlenet"><a class="markdownIt-Anchor" href="#含并行连接的网络googlenet"></a> 含并行连接的网络（GoogLeNet）</h1>
<p>GoogLeNet论文的重点是解决了多大的卷积核最合适的问题。</p>
<p>该论文的观点是，有时使用大小不同的卷积核组合是有利的。</p>
<h2 id="inception块"><a class="markdownIt-Anchor" href="#inception块"></a> Inception块</h2>
<p>Inception块由4条并写路径组成。</p>
<ul>
<li>前3条路径使用窗口大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>的卷积层，从不同的空间大小中提取信息</li>
<li>中间的两条路径在输入上执行<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积。以减小通道数，从而降低模型复杂度</li>
<li>第四条路径使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>最大汇聚层，然后使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层来改变通道数</li>
</ul>
<p>实现如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class Inception(nn.Module):<br>    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):<br>        super().__init__()<br>        <br>        self.p1 = nn.Conv2d(in_channels, c1, kernel_size=1)<br>        self.p2 = nn.Sequential(<br>            nn.Conv2d(in_channels, c2[0], kernel_size=1),<br>            nn.ReLU(),<br>            nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1),<br>            nn.ReLU()<br>        )<br>        self.p3 = nn.Sequential(<br>            nn.Conv2d(in_channels, c3[0], kernel_size=1),<br>            nn.ReLU(),<br>            nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2),<br>            nn.ReLU()<br>        )<br>        self.p4 = nn.Sequential(<br>            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),<br>            nn.Conv2d(in_channels, c4, kernel_size=1),<br>            nn.ReLU()<br>        )<br>    def forward(self, X):<br>        p1_result = self.p1(X)<br>        p2_result = self.p2(X)<br>        p3_result = self.p3(X)<br>        p4_result = self.p4(X)<br><br>        return torch.cat((p1_result, p2_result, p3_result, p4_result), dim=1)<br></code></pre></td></tr></table></figure>
<h2 id="goolenet模型"><a class="markdownIt-Anchor" href="#goolenet模型"></a> GooLeNet模型</h2>
<p>一共使用9个Inception块和全局平均汇聚层的堆叠类生成其估计值。</p>
<p>Inception块之间的最大汇聚层可以降低维度。</p>
<p>现在我们逐一实现GoogLeNet的每个模块，第一个模块使用64个通道、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7\times 7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span></span></span></span>卷积层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b1 = nn.Sequential(<br>    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),<br>    nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<br>)<br></code></pre></td></tr></table></figure>
<p>第二个模块使用两个卷积层：第一个卷积层是64个通道、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层；第二个卷积层使用将通道增加为3倍的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>卷积层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b2 = nn.Sequential(<br>    nn.Conv2d(64, 64, kernel_size=1),<br>    nn.ReLU(),<br>    nn.Conv2d(64, 192, kernel_size=3, padding=1),<br>    nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<br>)<br></code></pre></td></tr></table></figure>
<p>第三个模块串联两个完整的Inception块。</p>
<ul>
<li>第一个Inception块
<ul>
<li>输出通道数为：64，128，32，32，总共为256个通道</li>
<li>第二条路径和第三条路径首先将输入通道数分别减少到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>12</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
</ul>
</li>
<li>第二个Inception块
<ul>
<li>输出通道数为：128，192，96，64，总共为480个通道</li>
<li>第二条路径和第三条路径首先将输入通道分别减少到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b3 = nn.Sequential(<br>    Inception(192, 64, (96, 128), (16, 32), 32),<br>    Inception(256, 128, (128, 192), (32, 96), 64),<br>    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<br>)<br></code></pre></td></tr></table></figure>
<p>第四个模块，串联了5个Inception块，其输出通道分别为：192+208+48+64=512，160+224+64+64=512，128+256+64+64=512，112+288+64+64=528，256+320+128+128=832.</p>
<p>这些路径的通道数的分配与第三个模块相似，输出通道数最多的是含<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>卷积层的第二条路径，其次是仅含<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层的第一条路径，最后是含<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>卷积层的第三条路径和含<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>最大汇聚层的第四条路径。</p>
<p>其中第二条路径和第三条路径都会先按比例减少通道数，这些比例在各个Inception块中略有不同。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b4 = nn.Sequential(<br>    Inception(480, 192, (96, 208), (16, 48), 64),<br>    Inception(512, 160, (112, 224), (24, 64), 64),<br>    Inception(512, 128, (128, 256), (26, 64), 64),<br>    Inception(512, 112, (144, 288), (32, 64), 64),<br>    Inception(528, 256, (160, 320), (32, 128), 128),<br>    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<br>)<br></code></pre></td></tr></table></figure>
<p>第五个模块包含输出通道数为256+320+128+128=832和384+384+128+128=1024.</p>
<p>由于第五个模块后面紧跟输出层，所以该模块使用了全局平均汇聚层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b5 = nn.Sequential(<br>    Inception(832, 256, (160, 320), (32, 128), 128),<br>    Inception(832, 384, (192, 384), (48, 128), 128),<br>    nn.AdaptiveAvgPool2d((1,1)),<br>    nn.Flatten()<br>)<br><br>net = nn.Sequential(b1,b2,b3,b4,b5, nn.Linear(1024, 10))<br></code></pre></td></tr></table></figure>
<p>为了便于训练，将Fashion-MNIST的宽高改为96。</p>
<p>测试模型运行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.rand(size=(1,1,96,96))<br>for layer in net:<br>    X = layer(X)<br>    print(layer.__class__.__name__, X.shape)<br></code></pre></td></tr></table></figure>
<h2 id="训练模型-2"><a class="markdownIt-Anchor" href="#训练模型-2"></a> 训练模型</h2>
<p>如果出现虚拟内存不足，改小batch_size.</p>
<h1 id="批量规范化"><a class="markdownIt-Anchor" href="#批量规范化"></a> 批量规范化</h1>
<h2 id="训练深层网络"><a class="markdownIt-Anchor" href="#训练深层网络"></a> 训练深层网络</h2>
<p>训练神经网络的影响因素：</p>
<ul>
<li>数据预处理的方式通常会对最终结果产生巨大的影响。</li>
<li>对于典型的多层感知机或卷积神经网络，当训练时，中间层中的变量可能具有更大的变化范围，模型参数随着训练更新而变换莫测；这种偏移可能会阻碍网络的收敛。<s>网络层心不齐</s></li>
<li>更深层的网络很复杂，容易过拟合</li>
</ul>
<p>批量规范化应用于单个可选层：在每次训练迭代中，首先规范化输入，即减去其均值并处以标准差，这二者均基于当前小批量处理。接下来，应用比例系数和比例偏移。</p>
<p>只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。</p>
<p>从形式上来说，用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">x\in B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>表示一个来自小批量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>的输入，批量规范化<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">BN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>根据以下表达式转换<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>⊙</mo><mfrac><mrow><mi>x</mi><mo>−</mo><msub><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mi>B</mi></msub></mrow><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>B</mi></msub></mfrac><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">BN(x) = \gamma \odot \frac{x-\hat\mu_B}{\hat\sigma_B}+\beta
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span></span></p>
<p>式中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\hat\mu_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是小批量B的样本均值，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\hat\sigma_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是小批量B的样本标准差。应用标准化后，生成的小批量的均值为0，方差为1.</p>
<p>由于单位方差是一个主观的选择，因此我们通常包含拉伸参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>和偏移参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>。</p>
<p>需注意，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma, \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>是需要与其他模型参数一起学习的参数。</p>
<p>关于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mi>B</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\hat\mu_B, \hat\sigma_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的计算：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>B</mi></mrow></munder><mi>x</mi><mspace linebreak="newline"></mspace><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>B</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>B</mi></mrow></munder><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mi>B</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\hat\mu_B = \frac{1}{|B|}\sum_{x\in B}x\\
\hat\sigma_B = \frac{1}{|B|}\sum_{x\in B}(x-\hat\mu_B)^2+\epsilon
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.6431459999999998em;vertical-align:-1.321706em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.6431459999999998em;vertical-align:-1.321706em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span></span></p>
<p>注意，我们在方差的估计值添加一个小的常量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>，以确保永远不会除以零。</p>
<p>另外，优化中的各种噪声源通常会导致更快的训练和较少的过拟合，虽然目前尚未在理论上明确证明。</p>
<p>另外，批量规范化层在训练模型和预测模型中的功能不同：</p>
<ul>
<li>训练模式下，我们无法使用整个数据集来估计均值和方差，所以只能根据每个小批量的均值和方差不断训练模型。</li>
<li>在预测模式下，可以根据整个数据集精确计算批量规范化所需的均值和方差。</li>
</ul>
<h2 id="批量规范化层"><a class="markdownIt-Anchor" href="#批量规范化层"></a> 批量规范化层</h2>
<p>由于批量规范化在完整的小批量上执行，因此我们不能像之前在其他层那样忽略批量的大小。</p>
<h3 id="全连接层"><a class="markdownIt-Anchor" href="#全连接层"></a> 全连接层</h3>
<p>通常，我们将批量规范化置于全连接层中的仿射变换和激活函数之间。</p>
<p>设全连接层的输入为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>，权重参数和偏置参数分别为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">W, b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span></span></span></span>，激活函数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span>，批量规范化层为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">BN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>。</p>
<p>那么使用批量规范化的全连接层的输出的计算公式为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>B</mi><mi>N</mi><mo stretchy="false">(</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h = \phi(BN(Wx+b))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> 卷积层</h3>
<p>对于卷积层，我们可以在卷积层之后和非线性激活函数之间应用批量规范化。</p>
<p>当卷积具有多个输出通道时，每个通道都需要执行批量规范化，每个通道都有自己的拉伸参数和偏移参数，这两个参数都是标量。</p>
<h3 id="预测过程中的批量规范化"><a class="markdownIt-Anchor" href="#预测过程中的批量规范化"></a> 预测过程中的批量规范化</h3>
<ul>
<li>首先，我们不再需要样本均值中的噪声以及在微批量上估计每个小批量产生的样本方差。</li>
<li>其次，我们可能需要使用模型对逐个样本进行预测，一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。</li>
</ul>
<h2 id="从零实现"><a class="markdownIt-Anchor" href="#从零实现"></a> 从零实现</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def batch_norm(x, gamma, beta, moving_mean, moving_var, eps, momentum):<br>    if not torch.is_grad_enabled():<br>        # 预测模式下，使用移动平均所得的均值和方差<br>        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)<br>    else:<br>        # 训练模式<br>        assert len(X.shape) in (2, 4)<br>        if len(X.shape) == 2:<br>            # 使用全连接层<br>            mean = X.mean(dim=0)<br>            var = ((X-mean)**2).mean(dim=0)<br>        else:<br>            # 使用卷积层<br>            mean = X.mean(dim=(0, 2, 3), keepdim=True)<br>            var = ((X-mean)**2).mean(dim=(0,2,3), keepdim=True)<br>        X_hat = (X-mean)/torch.sqrt(eps+var)<br>        moving_mean = (1-momentum)*mean+momentum*moving_mean<br>        moving_var = (1-momentum)*var+momentum*moving_var<br><br>    Y = gamma*X_hat+beta<br>    return Y, moving_mean, moving_var<br></code></pre></td></tr></table></figure>
<p>接下来我们创建一个正确的BatchNorm层，这个层将保存拉伸参数和偏移参数，以及均值和方差的移动平均值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class BatchNorm(nn.Module):<br>    def __init__(self, num_features, dim):<br>        super().__init__()<br><br>        if dim == 2:<br>            shape = (1, num_features)<br>        else:<br>            shape = (1, num_features, 1, 1)<br>        <br>        self.gamma = nn.Parameter(torch.ones(size=shape, requires_grad=True))<br>        self.beta = nn.Parameter(torch.zeros(size=shape, requires_grad=True))<br>        self.moving_mean = torch.ones(size=shape)<br>        self.moving_var = torch.ones(size=shape)<br><br>    def forward(self, X):<br>        # 先看一下moving_mean和moving_var与X的设备是不是同一个<br>        if self.moving_mean.device != X.device:<br>            self.moving_mean.to(X.device)<br>            self.moving_var.to(X.device)<br>        <br>        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)<br><br>        return Y<br></code></pre></td></tr></table></figure>
<h2 id="使用批量规范化层的lenet"><a class="markdownIt-Anchor" href="#使用批量规范化层的lenet"></a> 使用批量规范化层的LeNet</h2>
<p>批量规范化应该应用于卷积、全连接层之后，相应激活函数之前。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = nn.Sequential(<br>    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, 4), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=2, stride=2),<br>    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, 4), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),<br>    nn.Linear(16*4*4, 120), BatchNorm(120, 2), nn.Sigmoid(),<br>    nn.Linear(120, 84), BatchNorm(84, 2), nn.Sigmoid(),<br>    nn.Linear(84, 10)<br>)<br></code></pre></td></tr></table></figure>
<p>训练过程与之前一致，只是学习率可以大得多。</p>
<h2 id="简明实现"><a class="markdownIt-Anchor" href="#简明实现"></a> 简明实现</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = nn.Sequential(<br>    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=2, stride=2),<br>    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),<br>    nn.Linear(16*4*4, 120), nn.BatchNorm1d(120), nn.Sigmoid(),<br>    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),<br>    nn.Linear(84, 10)<br>)<br></code></pre></td></tr></table></figure>
<p>通常高级API变体的运行速度快得多，因为它的代码已编译c++或CUDA。</p>
<h2 id="争议"><a class="markdownIt-Anchor" href="#争议"></a> 争议</h2>
<p>总结就是批量规范化很玄学。</p>
<h1 id="残差网络resnet"><a class="markdownIt-Anchor" href="#残差网络resnet"></a> 残差网络（ResNet）</h1>
<h2 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题</h2>
<p>随着网络越来越深：</p>
<ul>
<li>靠近输入的块在更新时，由于梯度较小，更新速度慢；而靠近输出的块变换速度快。</li>
<li>靠近输出的块没有及时得到有效的信息，其更新对损失影响小</li>
<li>损失值在很长的一段时间不能有效下降</li>
</ul>
<p>需想办法，降低梯度下降时的路径长度，避免出现梯度消失。</p>
<h2 id="函数类书上的理由"><a class="markdownIt-Anchor" href="#函数类书上的理由"></a> 函数类（书上的理由）</h2>
<p>关于这个概念的解释很长，在此写下自己的理解。</p>
<ul>
<li>函数类是函数的结合，这些函数具有相同的结构，不同之处在于参数的具体数值</li>
<li>对于深度学习而言，框架的可能性的集合就是函数类，我们希望通过训练，在这个类里面找到一个最接近真实函数的函数</li>
<li>在构建更复杂框架时，应尽量包含经过实践验证，有效的小框架</li>
<li>对于深度神经网络，如果我们能将新添加的层训练成恒等函数，新模型和原模型将同样有效</li>
</ul>
<h2 id="残差块"><a class="markdownIt-Anchor" href="#残差块"></a> 残差块</h2>
<p>残差映射在现实中往往更容易优化，因此我们将传统的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>函数改为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x)-x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>。</p>
<p>对于传统的块：</p>
<ol>
<li>卷积层</li>
<li>批量规范化层</li>
<li>激活函数</li>
<li>卷积层</li>
<li>批量规范化层</li>
<li>激活函数</li>
</ol>
<p>而残差块为：</p>
<ol>
<li>卷积层</li>
<li>批量规范化层</li>
<li>激活函数</li>
<li>卷积层</li>
<li>批量规范化层</li>
<li>与输入相加</li>
<li>激活函数</li>
</ol>
<p>ResNet沿用了VGG的卷积层设计：</p>
<ol>
<li>残差块里有两个相同输出通道数的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>卷积层，每个卷积层后接一个批量规范化层和一个ReLU激活函数</li>
<li>通过跨层数据通道，跳过两个卷积运算，将输入直接加载最后的ReLU激活函数之前
<ul>
<li>这样的设计要求两个卷积层的输出与输入相同</li>
<li>如果想改变通道数，需要引入额外的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层改变输入的形状</li>
</ul>
</li>
</ol>
<p>残差块的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import torch<br>from torch import nn<br>from torch.nn import functional as F<br><br>import torch<br>from torch import nn<br>from torch.nn import functional as F<br><br>class Residual(nn.Module):<br>    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):<br>        super().__init__()<br><br>        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)<br>        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)<br><br>        if use_1x1conv:<br>            # 统一了输出形状：(w-1)/stride, (h-1)/stride<br>            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)<br>        else:<br>            self.conv3 = None<br>        <br>        self.bn1 = nn.BatchNorm2d(num_channels)<br>        self.bn2 = nn.BatchNorm2d(num_channels)<br><br>    def forward(self, X):<br>        Y = self.bn2(self.conv2(F.relu(self.bn1(self.conv1(X)))))<br>        if self.conv3:<br>            X = self.conv3(X)<br>        Y += X<br>        return Y<br></code></pre></td></tr></table></figure>
<h2 id="resnet模型"><a class="markdownIt-Anchor" href="#resnet模型"></a> ResNet模型</h2>
<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64，步幅为2的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7\times 7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span></span></span></span>卷积层后，接步幅为2的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的最大汇聚层，不同之处在于增加了批量规范化层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b1 = nn.Sequential(<br>    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),<br>    nn.BatchNorm2d(64), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<br>)<br></code></pre></td></tr></table></figure>
<p>GoogLeNet在后面接了四个由Inception块的组成的模块。ResNet则使用四个由残差块组成的模块。</p>
<p>第一个模块的通道数与输入通道数一致。</p>
<p>之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并且高度、宽度减半。</p>
<p>需注意，在这里，第一个块没有使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>卷积层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def resnet_block(input_channels, output_channels, num_residuals, first_block=False):<br>    blk = []<br>    for i in range(num_residuals):<br>        if i == 0 and not first_block:<br>            blk.append(Residual(input_channels, output_channels, True, strides=2))<br>        else:<br>            blk.append(Residual(output_channels, output_channels))<br>    <br>    return blk<br></code></pre></td></tr></table></figure>
<p>构建四个模块</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">b2 = nn.Sequential(*resnet_block(64, 64, 2, True))<br>b3 = nn.Sequential(*resnet_block(64, 128, 2))<br>b4 = nn.Sequential(*resnet_block(128, 256, 2))<br>b5 = nn.Sequential(*resnet_block(256, 512, 2))<br></code></pre></td></tr></table></figure>
<p>最后，与GoogLeNet一样，加入全局平均汇聚层和全连接层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">net = nn.Sequential(<br>    b1, b2, b3, b4, b5<br>    nn.AdaptiveAvgPool2d((1, 1)),<br>    nn.Flatten(),<br>    nn.Linear(512, 10)<br>)<br></code></pre></td></tr></table></figure>
<p>验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.rand(size=(1, 1, 224, 224))<br>for layer in net:<br>    X = layer(X)<br>    print(layer.__class__.__name__, X.shape)<br></code></pre></td></tr></table></figure>
<h2 id="训练-2"><a class="markdownIt-Anchor" href="#训练-2"></a> 训练</h2>
<p>与之前一致，注意内存大小。</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2024/03/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%EF%BC%88%E4%BA%8C%EF%BC%89/"
      title="计算机视觉（二）"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        计算机视觉（二）
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2024/03/06/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
      title="卷积神经网络"
     >

    <p class="title-text">
      
        卷积神经网络
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">留言 </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":"cad9Un1DX7ZRoGqWGBbgsdvR-gzGzoHsz","appKey":"KdaYDEn7evWoAgBjACdAKPdK","placeholder":"(>_<)","pageSize":10,"highlight":true,"serverURLs":"https://cad9un1d.lc-cn-n1-shared.com","el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 PY H<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
