<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<link href="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.10.2/katex.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>Transformer | DayDreamer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="Transformer结构介绍与实现">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="https://daydreamerh.github.io/2024/04/27/Transformer/index.html">
<meta property="og:site_name" content="DayDreamer">
<meta property="og:description" content="Transformer结构介绍与实现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daydreamerh.github.io/2024/04/27/Transformer/banner.jpg">
<meta property="article:published_time" content="2024-04-27T03:15:02.000Z">
<meta property="article:modified_time" content="2024-04-27T03:20:48.378Z">
<meta property="article:author" content="DayDreamer">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://daydreamerh.github.io/2024/04/27/Transformer/banner.jpg">
  
    <link rel="alternate" href="/atom.xml" title="DayDreamer" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  
   
  <div id="main-grid" class="shadow   ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>DayDreamer </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/avatar.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">PY H </div>
      <div class="dot"></div>
      <div class="subtitle"> </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/DaydreamerH" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
                计算机操作系统
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">
                PyTorch学习
                <div class="category-count">10</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">
                杂七杂八
                <div class="category-count">7</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/">
                套接字编程
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%94%BF%E6%B2%BB%E8%AF%BE%E5%A4%8D%E4%B9%A0/">
                政治课复习
                <div class="category-count">5</div>
            </a>
        
            <a class="category-link" href="/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/">
                项目开发
                <div class="category-count">3</div>
            </a>
        </div>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-Transformer" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      
      
      
      
      
      
      <a class="article-gallery-img" rel="gallery_clxiqgm6x000z7gv1ecwp5scw">
        <img src="/2024/04/27/Transformer/banner.jpg" itemprop="image">
      </a>
    
  </div>
</div>

   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        Transformer
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-04-27T03:15:02.000Z" itemprop="datePublished">2024-04-27</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/">PyTorch学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            9.6k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <p>Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络。</p>
<h1 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h1>
<p>Transformer作为编码器-解码器架构，整体架构如下图：<br>
<img src="/2024/04/27/Transformer/structure.jpg" alt></p>
<p>Transformer是由编码器和解码器组成，编码器和解码器是基于自注意力的模块叠加而成的，源序列和目标输出序列的嵌入表示将加上位置编码，再分别输入编码器和解码器中。</p>
<p>Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层。</p>
<ul>
<li>第一个子层是多头自注意力汇聚</li>
<li>第二个子层是基于位置的前馈网络</li>
<li>每个子层都采用了残差链接
<ul>
<li>对于序列中任何位置的任何输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x\in R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，都要求满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">sublayer(x)\in R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">b</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，以便残差连接满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>+</mo><mi>s</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x+sublayer(x)\in R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">b</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>.</li>
<li>再残差连接的加法计算后，紧接着应用层规范化</li>
<li>因此，对于输入序列的每个位置，Transformer编码器都将输出一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>维表示向量</li>
</ul>
</li>
</ul>
<p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。</p>
<ul>
<li>除了编码器中描述的两个子层，解码器还在这两个子层之间插入了第三个子层，称为编码-解码器注意力层</li>
<li>再解码器自注意力中，查询、键和值都来自上一个解码器层的输出</li>
<li>解码器中的每个位置都只能考虑该位置之前的所有位置，这种掩蔽注意力保留了自回归属性，以确保预测仅依赖已生成的输出词元</li>
</ul>
<h2 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import math<br>import panda as pd<br>import torch<br>from torch import nn<br>from d2l import torch as d2l<br></code></pre></td></tr></table></figure>
<h3 id="基于位置的前馈神经网络"><a class="markdownIt-Anchor" href="#基于位置的前馈神经网络"></a> 基于位置的前馈神经网络</h3>
<p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机，这就是称前馈神经网络是基于位置的原因。</p>
<p>在下面的实现中，输入X的形状(批量大小，时间步数或序列长度，隐单元数或特征维度)将被一个两层的感知机变换成形状为(批量大小，时间步数，ffn_num_outputs)的输出张量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#@save<br>class PositionWiseFFN(nn.Module):<br>    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):<br>       super(PositionWiseFFN, self).__init__(**kwargs)<br>       self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)<br>       self.relu = nn.ReLU()<br>       self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)<br><br>    def forward(self, X):<br>        return self.dense2(self.relu(self.dense1(X))) <br></code></pre></td></tr></table></figure>
<p>下面的例子显示，改变张量的最内层维度的尺寸，会改变基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ffn = PositionWiseFFN(4, 4, 8)<br>ffn.eval()<br>ffn(torch.ones((2, 3, 4)))[0]<br></code></pre></td></tr></table></figure>
<h3 id="残差连接和层规范化"><a class="markdownIt-Anchor" href="#残差连接和层规范化"></a> 残差连接和层规范化</h3>
<p>批量规范化是在一个小批量的样本内对数据进行重新中心化和重新缩放的调整。</p>
<p>层规范化与批量规范化的目标相同，但层规范化是基于特征维度进行规范化，往往使用在自然语言处理中。</p>
<p>现在可以使用残差连接和层规范化来实现AddNorm类，暂退法也被作为正则化方法使用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class AddNorm(nn.Module):<br>    def __init__(self, normalized_shape, dropout, **kwargs):<br>        super(AddNorm, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br>        self.ln = nn.LayerNorm(normalized_shape)<br><br>    def forward(self, X, Y):<br>        return self.ln(self.dropout(Y)+X)<br></code></pre></td></tr></table></figure>
<p>残差连接要求两个输入的形状相同，以便在加法操作后输出张量的形状相同：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">add_norm = AddNorm([3, 4], 0.5)<br>add_norm.eval()<br>add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape<br></code></pre></td></tr></table></figure>
<h3 id="编码器"><a class="markdownIt-Anchor" href="#编码器"></a> 编码器</h3>
<p>有了Transformer编码器的基础组件，现在可以先实现编码器中的一个层。</p>
<p>下面的EncoderBlock类包含两个子层：多头注意力和基于位置的前馈神经网络，这两个子层都使用了残差连接和紧随的层规范化：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class EncoderBlock(nn.Module):<br>    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias = False, **kwargs):<br>        super(EncoderBlock, self).__init__(**kwargs)<br>        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)<br>        self.addnorm1 = AddNorm(norm_shape, dropout)<br>        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)<br>        self.addnorm2 = AddNorm(norm_shape, dropout)<br><br>    def forward(self, X, valid_lens):<br>        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))<br>        return self.addnorm2(Y, self.ffn(Y))<br><br><br>class MultiHeadAttention(nn.Module):<br>    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias = False, **kwargs):<br>        super(MultiHeadAttention, self).__init__(**kwargs)<br>        self.W_q = nn.Linear(query_size, num_hiddens, bias=use_bias)<br>        self.W_k = nn.Linear(key_size, num_hiddens, bias=use_bias)<br>        self.W_v = nn.Linear(value_size, num_hiddens, bias=use_bias)<br>        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=use_bias)<br>        self.attention = d2l.DotProductAttention(dropout)<br>        self.num_heads = num_heads<br>    <br>    def forward(self, queries, keys, values, valid_lens):<br>        queries = transpose_qkv(self.W_q(queries), self.num_heads)<br>        keys = transpose_qkv(self.W_k(keys), self.num_heads)<br>        values = transpose_qkv(self.W_v(values), self.num_heads)<br><br>        if valid_lens is not None:<br>            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)<br>        <br>        output = self.attention(queries, keys, values, valid_lens)<br>        output_concat = transpose_output(output, self.num_heads)<br><br>        return self.W_o(output_concat)<br><br><br>def transpose_qkv(X, num_heads):<br>    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)<br>    X = X.permute(0, 2, 1, 3)<br>    return X.reshape(-1, X.shape[2], X.shape[3])<br><br><br>def transpose_output(X, num_heads):<br>    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])<br>    X = X.permute(0, 2, 1, 3)<br>    return X.reshape(X.shape[0], X.shape[1], -1)<br></code></pre></td></tr></table></figure>
<p>Transformer编码器中的任何层都不会改变其输入的形状。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">X = torch.ones((2, 100, 24))<br>valid_lens = torch.tensor([3, 2])<br>encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)<br>encoder_blk.eval()<br>encoder_blk(X, valid_lens).shape<br></code></pre></td></tr></table></figure>
<p>下面实现的Transformer编码器的代码中，堆叠了num_layers个EncoderBlock类的实例。由于这里使用的是值范围在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>1</mn><mtext> </mtext><mn>1</mn></mrow><annotation encoding="application/x-tex">-1~1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mspace nobreak"> </span><span class="mord">1</span></span></span></span>的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，再与位置编码相加。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class TransformerEncoder(d2l.Encoder):<br>    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias = False, **kwargs):<br>        super(TransformerEncoder, self).__init__(**kwargs)<br>        self.num_hiddens = num_hiddens<br>        self.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        self.blks = nn.Sequential()<br>        for i in range(num_layers):<br>            self.blks.add_module(&quot;block&quot;+str(i), <br>                EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))<br>        <br>    def forward(self, X, valid_lens, *args):<br>        X = self.pos_encoding(self.embedding(x)*math.sqrt(self.num_hiddens))<br>        self.attention_weights = [None]*len(self.blks)<br>        for i, blk in enumerate(self.blks):<br>            X = blk(X, valid_lens)<br>            self.attention_weights[i] = blk.attention.attention_weights<br>        return X<br></code></pre></td></tr></table></figure>
<p>下面我们指定超参数来创建一个两层的Transformer编码器。Transformer编码器输出的形状是(批量大小，时间步数，num_hiddens)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">encoder = TransformerEncoder(<br>    200, 24, 24, 24, 24,[100, 24], 24, 48, 8, 2, 0.5)<br>encoder.eval()<br>encoder(torch.ones((2, 100), dtype= torch.long), valid_lens).shape<br></code></pre></td></tr></table></figure>
<h3 id="解码器"><a class="markdownIt-Anchor" href="#解码器"></a> 解码器</h3>
<p>在DecoderBlock类中，实现的每个层包含3个子层：解码器自注意力，编码器-解码器注意力和基于位置的前馈网络。</p>
<p>这些子层也都被残差连接和紧随的层规范化围绕。</p>
<p>正如前面所说，在掩蔽多头解码器自注意力层中，查询、键和值都来自上一个解码器的输出。</p>
<p>关于序列到序列模型，在训练阶段，其输出序列的所有位置的词元都是已知的：然而在预测阶段，其输出序列的词元是逐个生成的。因此在解码器的任何时间步中，只有生成的词元才能用于解码器的自注意力计算中。</p>
<p>为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数dec_valid_lens，以便任何查询都只会与解码器中所有已经生成词元的位置进行注意力计算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class DecoderBlock(nn.Module):<br>    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):<br>        super(DecoderBlock, self).__init__(**kwargs)<br>        self.i = i<br>        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)<br>        self.addnorm1 = AddNorm(norm_shape, dropout)<br>        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)<br>        self.addnorm2 = AddNorm(norm_shape, dropout)<br>        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)<br>        self.addnorm3 = AddNorm(norm_shape, dropout)<br>    <br>    def forward(self, X, state):<br>        enc_outputs, enc_valid_lens = state[0], state[1]<br>        # 训练阶段，输出序列的所有词元都在同一时间处理<br>        # 因此state[2][self.i]初始化为None<br>        # 预测阶段，输出序列是通过词元一个一个接着解码的<br>        # 因此state[2][self.i]包含直到当前时间步第i个块解码的输出表示<br>        if state[2][self.i] is None:<br>            key_values = X<br>        else:<br>            key_values = torch.cat((state[2][self.i], X), axis = 1)<br>        state[2][self.i] = key_values<br>        if self.training :<br>            batch_size, num_steps, _ = X.shape<br>            dec_valid_lens = torch.arange(<br>                1, num_steps+1, device=X.device).repeat(batch_size, 1)<br>        else:<br>            dec_valid_lens = None<br>        <br>        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)<br>        Y = self.addnorm1(X, X2)<br>        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)<br>        Z = self.addnorm2(Y, Y2)<br>        return self.addnorm3(Z, self.ffn(Z)), state<br></code></pre></td></tr></table></figure>
<p>为了便于在编码器-解码器注意力中进行缩放点积运算和在残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)<br>decoder_blk.eval()<br>X = torch.ones((2, 100, 24))<br>state = [encoder_blk(X, valid_lens), valid_lens, [None]]<br>decoder_blk(X, state)[0].shape<br></code></pre></td></tr></table></figure>
<p>现在我们构建Transformer解码器，最后通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器自注意力权重和编码器-解码器注意力权重都被存储下来，以便日后可视化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">class TransformerDecoder(d2l.AttentionDecoder):<br>    def __init__(self, vocab_size, key_size, query_size, value_size,<br>                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,<br>                 num_heads, num_layers, dropout, **kwargs):<br>        super(TransformerDecoder, self).__init__(**kwargs)<br>        self.num_hiddens = num_hiddens<br>        self.num_layers = num_layers<br>        self.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        self.blks = nn.Sequential()<br>        for i in range(num_layers):<br>            self.blks.add_module(&quot;block&quot;+str(i),<br>                DecoderBlock(key_size, query_size, value_size, num_hiddens,<br>                             norm_shape, ffn_num_input, ffn_num_hiddens,<br>                             num_heads, dropout, i))<br>        self.dense = nn.Linear(num_hiddens, vocab_size)<br><br>    def init_state(self, enc_outputs, enc_valid_lens, *args):<br>        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]<br><br>    def forward(self, X, state):<br>        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))<br>        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]<br>        for i, blk in enumerate(self.blks):<br>            X, state = blk(X, state)<br>            # 解码器自注意力权重<br>            self._attention_weights[0][<br>                i] = blk.attention1.attention.attention_weights<br>            # “编码器－解码器”自注意力权重<br>            self._attention_weights[1][<br>                i] = blk.attention2.attention.attention_weights<br>        return self.dense(X), state<br><br>    @property<br>    def attention_weights(self):<br>        return self._attention_weights<br></code></pre></td></tr></table></figure>
<h3 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h3>
<pre class="highlight"><code class>num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32,32
norm_shape = [32]

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout)
decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout)

class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)

net = EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)```
</code></pre>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2024/05/14/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/"
      title="数字图像考试复习"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        数字图像考试复习
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2024/04/25/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"
      title="注意力机制"
     >

    <p class="title-text">
      
        注意力机制
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">留言 </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":"cad9Un1DX7ZRoGqWGBbgsdvR-gzGzoHsz","appKey":"KdaYDEn7evWoAgBjACdAKPdK","placeholder":"(>_<)","pageSize":10,"highlight":true,"serverURLs":"https://cad9un1d.lc-cn-n1-shared.com","el":"#vcomments"});
  </script>



    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2024 PY H<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
